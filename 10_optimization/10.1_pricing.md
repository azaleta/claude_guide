## 10.1 Token 计费原理与成本模型

在 AI 时代，Token 就是新的电力。
理解 Token 的计费逻辑，是每一位 AI 工程师的基本功。这直接决定了商业模式是否成立。

### 10.1.1 什么是 Token？

Token 并不等同于单词（Word）或字符（Character）。它是 LLM 处理文本的最小颗粒。
*   **英文**: 1 Token ≈ 0.75 单词。 "Apple" = 1 token.
*   **中文**: 1 Token ≈ 0.5 - 0.7 汉字。由于 UTF-8 编码，中文通常比英文消耗更多 Token。
    *   “你好” ≈ 3-4 Tokens。
    *   “人工智能” ≈ 4-6 Tokens。

**实战测试**:
可以使用官方的 Tokenizer 工具或 Python 库来精确计算。

### 10.1.2 计费公式

Anthropic 的计费通常分为两部分：
$$ \text{Total Cost} = (\text{Input Tokens} \times P_{in}) + (\text{Output Tokens} \times P_{out}) $$

通常 $P_{out}$ (生成) 的价格是 $P_{in}$ (阅读) 的 3-5 倍。
**这意味着**:
*   **读**文档很便宜。
*   **写**文章很贵。

### 10.1.3 三档模型成本对比 (2026 参考价)

虽然具体价格会变动，但相对比例通常保持稳定。以 **Haiku 单位 (HU)** 来表示相对成本。

| 模型                    | Input Cost    | Output Cost   | 性能定位    | 相对成本        |
| :-------------------- | :------------ | :------------ | :------ | :---------- |
| **Claude 3.5 Haiku**  | $1.00 / MTok  | $5.00 / MTok  | 极速、轻量   | **1x (基准)** |
| **Claude 4.5 Sonnet** | $3.00 / MTok  | $15.00 / MTok | 均衡、SOTA | **3x**      |
| **Claude 4 Opus**     | $15.00 / MTok | $75.00 / MTok | 深度推理    | **15x**     |

**结论**:
*   Opus 比 Haiku 贵 15 倍。
*   虽差距缩小，但在大规模并发场景下，3 倍的成本差异依然显著。

### 10.1.4 隐藏成本 (Hidden Costs)

在计算 ROI 时，除了 API 费用，还要考虑：
1.  **思维链消耗 (CoT Overhead)**: 为了提高准确率，常让模型 "Think step by step"。这会产生额外的 500-1000 Output Tokens。
2.  **错误重试 (Retry)**: Agent 运行失败重试的成本。
3.  **Context 膨胀**: 多轮对话中，历史记录越来越长，每一轮的 Input Cost 都在指数级增长。

### 10.1.5 成本计算案例：客服机器人

假设一个客服机器人平均每天接待 1000 人，每人对话 10 轮。
*   平均每轮 Input (含历史): 2000 Tokens (RAG + History).
*   平均每轮 Output: 200 Tokens.

**使用 Sonnet**:
$$ 1000 \times 10 \times (2000 \times 3 + 200 \times 15) / 1,000,000 = \$90 / \text{day} $$
**年成本**: $32,850。

**使用 Haiku**:
$$ 1000 \times 10 \times (2000 \times 1.00 + 200 \times 5.00) / 1,000,000 = \$30 / \text{day} $$
**年成本**: $10,950。

**策略**: Haiku 相比 Sonnet 能节省 66% 的成本。因此对于简单问题，要优先考虑 Haiku 模型。

---

由于 Input Token 往往占据成本的大头（尤其是 RAG 场景），Anthropic 推出了一项革命性技术——**Prompt Caching**，来解决这个问题。

➡️ [Prompt Caching 提示缓存](10.2_caching.md)
